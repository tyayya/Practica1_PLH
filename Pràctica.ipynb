{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3208a31d",
   "metadata": {},
   "source": [
    "- P(w|wi-n+1) = Ct+lambda/N+B*lambda\n",
    "on Ct es el numero d'aparicions del enegrama, N \n",
    "\n",
    "Només es cert en cas de que aquest caracter depengui dels n-1 anteriors. Quant mes gran sigui la sequencia, més real es aixó. El problema si el enegrama es massa gran el Ct serà cada cop més petit.  Si és més petit, és menys precís.\n",
    "\n",
    "Fem servir smoothing pq hi ha enegramas que no estan al nostre corpus. \n",
    "\n",
    "- Posar doble espai al final i al inici del document per poder predir quin es el enegrama que indica l'inici/final (--a/a--)\n",
    "\n",
    "- Altra tècnica de smoothing: Interpolació P(wi wi-1 wi-2) = lambda0 * P(wi, wi-1, wi-2) + lambda1 * P(wi, wi-1) + lambda2 * P(wi) on lambda0 + lambda1 + lambda2 = 1\n",
    "\n",
    "El pes de cada lambda depén de quina es la millor probabilitat donat el corpus. La P basada en enegrames més llargs és més precisa pero si el corpus es mes petit, anira malament...\n",
    "\n",
    "- Altra: Discount\n",
    "\n",
    "Pabs(X =x) = (C(X=x)-delta)/N if C(w1...wn) > 0\n",
    "            ((B-No)*delta/No)/N otherwise\n",
    "\n",
    "- Altra: Backing-off\n",
    "\n",
    "Si trobem el enegrama fem servir la seva prob. Si no, la del n-1 grama. Si no, la del n-2 grama...\n",
    "S'han d'aproximar una certa alfa i una certa belta per aasegurar que el sumatori de les probabilitats siguin 1.\n",
    "Es poden donar valors aporximats i ja està, normalment es farien amb good tuning.\n",
    "\n",
    "- Probar con diferente numero de enegramas de cara al análisis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184f6e6",
   "metadata": {},
   "source": [
    "**Càrrega de dades**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee12e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8653678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(r\"C:\\Users\\Aya\\UPC\\PLH\\Projecte 1\\langId\\eng_trn.txt\")\n",
    "text = path.read_text(encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d105f7",
   "metadata": {},
   "source": [
    "**Preprocés**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d1a5b",
   "metadata": {},
   "source": [
    "***Eliminar els dígits***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f75b2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text_clean = re.sub(r\"\\d+\", \"\", text)  # eliminar números\n",
    "\n",
    "text = text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab88c3c",
   "metadata": {},
   "source": [
    "***Convertir el text a minúscula***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25fb34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()  # convertir a minúsculas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f151512",
   "metadata": {},
   "source": [
    "***Substituir espais en blanc per un sol***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "452fa216",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r\"\\s+\", \" \", text).strip()  # eliminar espacios extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d621de",
   "metadata": {},
   "source": [
    "**Eliminar caràcters extranys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_chars = [\"—\", \"…\", ':', \"“\", \"”\", \"‘\", \"’\", \"•\", \"–\", \"—\", \"―\", \"«\", \"»\", '%', '&', '$', '@', '(', ')', '[', ']', '{', '}', '|', '\\\\', '/', '<', '>', '^', '~', '`']\n",
    "for char in weird_chars:\n",
    "    text = text.replace(char, \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298eb38",
   "metadata": {},
   "source": [
    "***Concatenar totes les frases amb un espai doble al mig***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2052252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". .  how the mighty have fallen. . .  major companies coming out with their latest quarterly numbers include dave   buster's, men's wearhouse, palo alto networks, box, barnes   noble, pep boys-manny, moe   jack, ollie's bargain outlet, davidstea, lululemon athletica and kroger. january , magazine why are there so many magna cartas? : now watching up next word association with brad woodhouse the president of the democratic american bridge pac on the gop presidential field. : autoplay autoplay cop\n"
     ]
    }
   ],
   "source": [
    "sentences = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "text_joined = \"  \".join(sentences)\n",
    "text = text_joined\n",
    "print(text[:500])  # vistazo rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9fc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# ─── Función de preprocesado completa ───────────────────────────────────\n",
    "\n",
    "weird_chars = [\n",
    "    \"—\", \"…\", \":\", \"\\u201c\", \"\\u201d\", \"\\u2018\", \"\\u2019\", \"•\", \"–\",\n",
    "    \"―\", \"«\", \"»\", \"%\", \"&\", \"$\", \"@\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\",\n",
    "    \"|\", \"\\\\\", \"/\", \"<\", \">\", \"^\", \"~\", \"`\", \"#\", \"=\", \"+\", \"*\", \"_\",\n",
    "]\n",
    "\n",
    "def preprocess(raw_text):\n",
    "    \"\"\"Aplica todos los pasos de preprocesado al texto crudo.\"\"\"\n",
    "    processed = []\n",
    "    for line in raw_text.splitlines():\n",
    "        line = re.sub(r\"\\d+\", \"\", line)          # eliminar dígitos\n",
    "        line = line.lower()                       # minúsculas\n",
    "        for ch in weird_chars:\n",
    "            line = line.replace(ch, \" \")\n",
    "        line = re.sub(r\"\\s+\", \" \", line).strip()  # colapsar espacios\n",
    "        if line:\n",
    "            processed.append(line)\n",
    "    # Concatenar frases con doble espacio (marca de frontera)\n",
    "    return \"  \".join(processed)\n",
    "\n",
    "# ─── Cargar y preprocesar todos los idiomas ─────────────────────────────\n",
    "\n",
    "languages = [\"deu\", \"eng\", \"fra\", \"ita\", \"nld\", \"spa\"]\n",
    "base_path = Path(\"langId\")\n",
    "\n",
    "train_texts = {}\n",
    "test_texts  = {}\n",
    "\n",
    "for lang in languages:\n",
    "    train_texts[lang] = preprocess((base_path / f\"{lang}_trn.txt\").read_text(encoding=\"utf-8\"))\n",
    "    test_texts[lang]  = preprocess((base_path / f\"{lang}_tst.txt\").read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print(\"Idiomas cargados:\")\n",
    "for lang in languages:\n",
    "    print(f\"  {lang}  →  train {len(train_texts[lang]):>8,} chars | test {len(test_texts[lang]):>7,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e0f43",
   "metadata": {},
   "source": [
    "**Divisió en trigrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5277b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Funciones de extracción de n-gramas ─────────────────────────────────\n",
    "\n",
    "def extract_ngrams(text, n):\n",
    "    \"\"\"Extrae n-gramas de caracteres con padding de espacios.\"\"\"\n",
    "    # Padding: (n-1) espacios al principio y al final\n",
    "    padded = \" \" * (n - 1) + text + \" \" * (n - 1)\n",
    "    return [padded[i:i+n] for i in range(len(padded) - n + 1)]\n",
    "\n",
    "def build_ngram_counts(text, max_n=3):\n",
    "    \"\"\"Construye conteos de n-gramas para órdenes 1 hasta max_n.\"\"\"\n",
    "    counts = {}\n",
    "    for n in range(1, max_n + 1):\n",
    "        counts[n] = Counter(extract_ngrams(text, n))\n",
    "    return counts\n",
    "\n",
    "# ─── Construir conteos de trigramas para cada idioma ────────────────────\n",
    "\n",
    "lang_ngrams = {}\n",
    "for lang in languages:\n",
    "    lang_ngrams[lang] = build_ngram_counts(train_texts[lang], max_n=3)\n",
    "\n",
    "print(\"Trigramas antes del filtrado:\")\n",
    "for lang in languages:\n",
    "    n_unique = len(lang_ngrams[lang][3])\n",
    "    n_total  = sum(lang_ngrams[lang][3].values())\n",
    "    print(f\"  {lang}  →  {n_unique:>6,} únicos  |  {n_total:>10,} totales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bcfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Filtrar n-gramas con menos de 5 apariciones ────────────────────────\n",
    "\n",
    "MIN_COUNT = 5\n",
    "\n",
    "def filter_ngrams(ngram_counts, min_count=MIN_COUNT):\n",
    "    \"\"\"Elimina n-gramas con menos de min_count apariciones.\"\"\"\n",
    "    filtered = {}\n",
    "    for n, counts in ngram_counts.items():\n",
    "        filtered[n] = Counter({k: v for k, v in counts.items() if v >= min_count})\n",
    "    return filtered\n",
    "\n",
    "for lang in languages:\n",
    "    lang_ngrams[lang] = filter_ngrams(lang_ngrams[lang])\n",
    "\n",
    "print(f\"Trigramas después del filtrado (min_count={MIN_COUNT}):\")\n",
    "for lang in languages:\n",
    "    n_unique = len(lang_ngrams[lang][3])\n",
    "    n_total  = sum(lang_ngrams[lang][3].values())\n",
    "    print(f\"  {lang}  →  {n_unique:>6,} únicos  |  {n_total:>10,} totales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Vocabulario compartido ──────────────────────────────────────────────\n",
    "\n",
    "all_vocab = set()\n",
    "for lang in languages:\n",
    "    all_vocab.update(set(train_texts[lang]))\n",
    "\n",
    "B = len(all_vocab)   # tamaño del alfabeto (número de caracteres únicos)\n",
    "\n",
    "print(f\"Tamaño del vocabulario (B) = {B}\")\n",
    "print(f\"Caracteres: {''.join(sorted(all_vocab))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef1805",
   "metadata": {},
   "source": [
    "**Model d'enegrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a21f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Modelo de Lidstone ──────────────────────────────────────────────────\n",
    "# P(c_i | c_{i-2} c_{i-1}) = (C(trigrama) + λ) / (C(contexto_bigrama) + B · λ)\n",
    "#\n",
    "# λ ∈ (0, 1]:  λ=1 → suavizado de Laplace;  λ→0 → MLE puro.\n",
    "# B = tamaño del vocabulario (caracteres únicos).\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class LidstoneModel:\n",
    "    \"\"\"Modelo de lenguaje de trigramas con suavizado de Lidstone.\"\"\"\n",
    "\n",
    "    def __init__(self, ngram_counts, vocab_size, lam=0.1):\n",
    "        self.tri   = ngram_counts[3]\n",
    "        self.bi    = ngram_counts[2]\n",
    "        self.B     = vocab_size\n",
    "        self.lam   = lam\n",
    "\n",
    "    def log_prob(self, text):\n",
    "        \"\"\"Log-probabilidad del texto completo.\"\"\"\n",
    "        n = 3\n",
    "        padded = \" \" * (n - 1) + text + \" \" * (n - 1)\n",
    "        total = 0.0\n",
    "        for i in range(len(padded) - n + 1):\n",
    "            trigram = padded[i:i+n]\n",
    "            context = trigram[:2]                     # bigrama contexto\n",
    "            ct  = self.tri.get(trigram, 0)\n",
    "            ctx = self.bi.get(context, 0)\n",
    "            # Lidstone: (C + λ) / (C_ctx + B·λ)\n",
    "            prob = (ct + self.lam) / (ctx + self.B * self.lam)\n",
    "            total += math.log(prob)\n",
    "        return total\n",
    "\n",
    "print(\"Clase LidstoneModel definida ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Modelo de Backing-off ───────────────────────────────────────────────\n",
    "# Si C(trigrama) > 0 → usar P_tri = C(tri)/C(ctx_bi)\n",
    "# Si no, backoff a bigrama:   α · P_bi  = α · C(bi)/C(ctx_uni)\n",
    "# Si no, backoff a unigrama:  β · P_uni = β · (C(uni)+1)/(N_uni+B)   (Laplace en unigrama)\n",
    "#\n",
    "# α y β son factores de descuento que redistribuyen masa de probabilidad\n",
    "# a los n-gramas de orden inferior.\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class BackoffModel:\n",
    "    \"\"\"Modelo de lenguaje con backing-off: trigrama → bigrama → unigrama.\"\"\"\n",
    "\n",
    "    def __init__(self, ngram_counts, vocab_size, alpha=0.4, beta=0.4):\n",
    "        self.tri   = ngram_counts[3]\n",
    "        self.bi    = ngram_counts[2]\n",
    "        self.uni   = ngram_counts[1]\n",
    "        self.B     = vocab_size\n",
    "        self.alpha = alpha\n",
    "        self.beta  = beta\n",
    "        self.N_uni = sum(self.uni.values())\n",
    "\n",
    "    def log_prob(self, text):\n",
    "        \"\"\"Log-probabilidad del texto con backing-off.\"\"\"\n",
    "        n = 3\n",
    "        padded = \" \" * (n - 1) + text + \" \" * (n - 1)\n",
    "        total = 0.0\n",
    "        for i in range(len(padded) - n + 1):\n",
    "            trigram  = padded[i:i+n]\n",
    "            ctx_bi   = trigram[:2]          # contexto bigrama  (c_{i-2} c_{i-1})\n",
    "            bigram   = trigram[1:]          # bigrama           (c_{i-1} c_i)\n",
    "            ctx_uni  = trigram[1]           # contexto unigrama (c_{i-1})\n",
    "            unigram  = trigram[2]           # unigrama          (c_i)\n",
    "\n",
    "            ct_tri    = self.tri.get(trigram, 0)\n",
    "            ct_ctx_bi = self.bi.get(ctx_bi, 0)\n",
    "\n",
    "            if ct_tri > 0 and ct_ctx_bi > 0:\n",
    "                # ── Nivel 1: trigrama ────────\n",
    "                prob = ct_tri / ct_ctx_bi\n",
    "            else:\n",
    "                ct_bi      = self.bi.get(bigram, 0)\n",
    "                ct_ctx_uni = self.uni.get(ctx_uni, 0)\n",
    "                if ct_bi > 0 and ct_ctx_uni > 0:\n",
    "                    # ── Nivel 2: backoff a bigrama ──\n",
    "                    prob = self.alpha * (ct_bi / ct_ctx_uni)\n",
    "                else:\n",
    "                    # ── Nivel 3: backoff a unigrama (con add-1) ──\n",
    "                    ct_uni = self.uni.get(unigram, 0)\n",
    "                    prob = self.beta * ((ct_uni + 1) / (self.N_uni + self.B))\n",
    "\n",
    "            total += math.log(prob) if prob > 0 else math.log(1e-10)\n",
    "        return total\n",
    "\n",
    "print(\"Clase BackoffModel definida ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311882dd",
   "metadata": {},
   "source": [
    "**Experimentació: selecció del millor λ (Lidstone)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Experimentación: Lidstone con diferentes λ ─────────────────────────\n",
    "# Evaluamos accuracy de identificación de idioma sobre las frases de test.\n",
    "\n",
    "lambdas_to_test = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "N_SENTS = 100  # frases de test por idioma (para reducir tiempo)\n",
    "\n",
    "lidstone_results = {}\n",
    "\n",
    "for lam in lambdas_to_test:\n",
    "    # Construir un modelo Lidstone por idioma\n",
    "    models = {lang: LidstoneModel(lang_ngrams[lang], B, lam=lam) for lang in languages}\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    for true_lang in languages:\n",
    "        sentences = [s.strip() for s in test_texts[true_lang].split(\"  \") if len(s.strip()) >= 10]\n",
    "        for sent in sentences[:N_SENTS]:\n",
    "            predicted = max(languages, key=lambda l: models[l].log_prob(sent))\n",
    "            correct += (predicted == true_lang)\n",
    "            total   += 1\n",
    "\n",
    "    acc = correct / total\n",
    "    lidstone_results[lam] = acc\n",
    "    print(f\"  λ = {lam:<6}  →  Accuracy = {acc:.4f}  ({correct}/{total})\")\n",
    "\n",
    "best_lam = max(lidstone_results, key=lidstone_results.get)\n",
    "print(f\"\\n  ★ Mejor λ = {best_lam}  con accuracy = {lidstone_results[best_lam]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09abd6",
   "metadata": {},
   "source": [
    "**Experimentació: selecció de α i β (Backing-off)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Experimentación: Backing-off con diferentes α, β ────────────────────\n",
    "\n",
    "alpha_beta_pairs = [\n",
    "    (0.2, 0.05), (0.3, 0.1), (0.4, 0.1),\n",
    "    (0.4, 0.4),  (0.5, 0.2), (0.6, 0.3),\n",
    "]\n",
    "\n",
    "backoff_results = {}\n",
    "\n",
    "for alpha, beta in alpha_beta_pairs:\n",
    "    models = {lang: BackoffModel(lang_ngrams[lang], B, alpha=alpha, beta=beta) for lang in languages}\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    for true_lang in languages:\n",
    "        sentences = [s.strip() for s in test_texts[true_lang].split(\"  \") if len(s.strip()) >= 10]\n",
    "        for sent in sentences[:N_SENTS]:\n",
    "            predicted = max(languages, key=lambda l: models[l].log_prob(sent))\n",
    "            correct += (predicted == true_lang)\n",
    "            total   += 1\n",
    "\n",
    "    acc = correct / total\n",
    "    backoff_results[(alpha, beta)] = acc\n",
    "    print(f\"  α={alpha}, β={beta}  →  Accuracy = {acc:.4f}  ({correct}/{total})\")\n",
    "\n",
    "best_ab = max(backoff_results, key=backoff_results.get)\n",
    "print(f\"\\n  ★ Mejor (α, β) = {best_ab}  con accuracy = {backoff_results[best_ab]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da793092",
   "metadata": {},
   "source": [
    "### Justificació dels valors de N i B\n",
    "\n",
    "**N = 3 (trigrames de caràcters):**\n",
    "\n",
    "- Els trigrames capturen patrons locals característics de cada idioma (p.ex. *\"sch\"* en alemany, *\"the\"* en anglès, *\"les\"* en francès, *\"ció\"* en espanyol).\n",
    "- Els bigrames (N=2) tenen poca capacitat discriminativa: moltes parelles de caràcters són compartides entre idiomes.\n",
    "- Els 4-grames (N=4) serien més específics però pateixen de *data sparsity*: molts 4-grames apareixen 0 o 1 vegades, especialment en corpus petits, la qual cosa augmenta la dependència del suavitzat i pot empitjorar la generalització.\n",
    "- N=3 és l'estàndard en la identificació automàtica d'idiomes basada en caràcters (*Cavnar & Trenkle, 1994*) perquè ofereix el millor equilibri entre **especificitat** (prou context per diferenciar idiomes) i **cobertura** (suficients observacions per estimar probabilitats fiables).\n",
    "\n",
    "**B = |V| (mida del vocabulari de caràcters):**\n",
    "\n",
    "- B és el nombre de caràcters únics presents en el corpus conjunt de tots els idiomes.\n",
    "- S'utilitza un vocabulari **compartit** perquè volem que tots els models siguin comparables: així, un caràcter no vist en un idioma no rep probabilitat 0 gràcies al suavitzat.\n",
    "- En la fórmula de Lidstone: $P(c_i \\mid c_{i-2}c_{i-1}) = \\frac{C(\\text{tri}) + \\lambda}{C(\\text{ctx}) + B \\cdot \\lambda}$, B controla quanta massa de probabilitat es redistribueix als n-grames no observats.\n",
    "- Un B gran (molts caràcters: accents, diacrítics) dilueix més la probabilitat, però reflecteix millor la diversitat real dels idiomes europeus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
